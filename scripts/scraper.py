import requests
from bs4 import BeautifulSoup
import os
import re


def fetch_and_parse_html(url):
    """
    URLã‹ã‚‰HTMLã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚¹ã™ã‚‹é–¢æ•°

    Args:
        url (str): å–å¾—ã™ã‚‹URL

    Returns:
        BeautifulSoup: ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸHTMLã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    try:
        # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
        response = requests.get(url, timeout=10)
        response.raise_for_status()  # HTTPã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ä¾‹å¤–ã‚’ç™ºç”Ÿ

        # HTMLã‚’ãƒ‘ãƒ¼ã‚¹
        soup = BeautifulSoup(response.content, "html.parser")

        return soup

    except requests.exceptions.RequestException as e:
        print(f"URLã®å–å¾—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None
    except Exception as e:
        print(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None


def estimate_claude_tokens(text):
    """
    Claudeç”¨ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°æ¦‚ç®—é–¢æ•°

    Claudeï¼ˆæ—¥æœ¬èªï¼‰ã®å ´åˆï¼š
    - ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠ: ç´„1.5æ–‡å­—/ãƒˆãƒ¼ã‚¯ãƒ³
    - æ¼¢å­—: ç´„1æ–‡å­—/ãƒˆãƒ¼ã‚¯ãƒ³
    - è‹±æ•°å­—: ç´„4æ–‡å­—/ãƒˆãƒ¼ã‚¯ãƒ³
    - HTML: ç´„3æ–‡å­—/ãƒˆãƒ¼ã‚¯ãƒ³

    Args:
        text (str): è¨ˆç®—å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        int: æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°
    """
    if not text:
        return 0

    # æ–‡å­—ç¨®åˆ¥ã«ã‚«ã‚¦ãƒ³ãƒˆ
    hiragana_katakana = len(
        [c for c in text if "\u3040" <= c <= "\u309f" or "\u30a0" <= c <= "\u30ff"]
    )
    kanji = len([c for c in text if "\u4e00" <= c <= "\u9faf"])
    ascii_chars = len([c for c in text if ord(c) < 128])
    other_chars = len(text) - hiragana_katakana - kanji - ascii_chars

    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®š
    estimated_tokens = (
        hiragana_katakana / 1.5
        + kanji / 1.0
        + ascii_chars / 4.0
        + other_chars / 2.0  # ãã®ä»–ã®æ–‡å­—
    )
    return int(estimated_tokens)


def extract_and_compress_content_bs4(
    soup, target_selector="section.entry-content", source_url=None
):
    """
    BeautifulSoupã‚’ä½¿ç”¨ã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆã‚ˆã‚ŠæŸ”è»ŸãªæŠ½å‡ºãŒå¯èƒ½ï¼‰
    ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ»ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ãƒ»URLï¼‰ã‚’å…ˆé ­ã«é…ç½®

    Args:
        soup (BeautifulSoup): ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸHTMLã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        target_selector (str): æŠ½å‡ºå¯¾è±¡ã®CSSã‚»ãƒ¬ã‚¯ã‚¿
        source_url (str): ã‚½ãƒ¼ã‚¹URL

    Returns:
        str: åœ§ç¸®ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰
    """

    try:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        title = soup.find("title")
        meta_description = soup.find("meta", attrs={"name": "description"})

        # ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        title_text = title.get_text(strip=True) if title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
        description_text = ""
        if meta_description and meta_description.get("content"):
            description_text = meta_description.get("content").strip()

        print(f"ğŸ“„ ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {title_text}")
        print(f"ğŸ”— ã‚½ãƒ¼ã‚¹URL: {source_url}")
        print(
            f"ğŸ“ ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³: {description_text[:100]}{'...' if len(description_text) > 100 else ''}"
        )

        # CSSã‚»ãƒ¬ã‚¯ã‚¿ã§è¦ç´ ã‚’æ¤œç´¢
        target_elements = soup.select(target_selector)

        if not target_elements:
            print(
                f"æŒ‡å®šã•ã‚ŒãŸã‚»ãƒ¬ã‚¯ã‚¿ '{target_selector}' ã«è©²å½“ã™ã‚‹è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
            )
            return ""

        # æœ€åˆã®è¦ç´ ã‚’ä½¿ç”¨
        target_element = target_elements[0]

        # åœ§ç¸®å‰ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—ï¼ˆå…ƒã®ãƒšãƒ¼ã‚¸å…¨ä½“ï¼‰
        original_full_tokens = estimate_claude_tokens(str(soup))
        # æŠ½å‡ºå¯¾è±¡è¦ç´ ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—ï¼ˆæŠ½å‡ºå¾Œï¼‰
        extracted_tokens = estimate_claude_tokens(str(target_element))

        # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤ï¼ˆscriptã‚¿ã‚°ã€styleã‚¿ã‚°ãªã©ï¼‰
        for tag in target_element(["script", "style", "noscript"]):
            tag.decompose()

        # imgã‚¿ã‚°ã®altå±æ€§ã‚’å‡¦ç†ï¼ˆaltå±æ€§ã‚’ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦æ®‹ã™ï¼‰
        img_tags = target_element.find_all("img", alt=True)
        for img in img_tags:
            alt_text = img.get("alt", "")
            if alt_text:
                # imgã‚¿ã‚°ã®å¾Œã«altå±æ€§ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
                img.insert_after(f" {alt_text} ")
            # srcå±æ€§ä»¥å¤–ã®å±æ€§ã‚’å‰Šé™¤ã—ã¦HTMLã‚’è»½é‡åŒ–
            img.attrs = {}

        # å…¨ã¦ã®è¦ç´ ã‹ã‚‰ä¸è¦ãªå±æ€§ã‚’å‰Šé™¤ï¼ˆã‚¯ãƒ©ã‚¹ã€IDã€ã‚¹ã‚¿ã‚¤ãƒ«ãªã©ï¼‰
        for tag in target_element.find_all(True):
            # åŸºæœ¬çš„ãªã‚¿ã‚°ã®ã¿ä¿æŒ
            if tag.name in ["a"]:
                # ãƒªãƒ³ã‚¯ã®hrefå±æ€§ã®ã¿ä¿æŒ
                href = tag.get("href")
                tag.attrs.clear()
                if href:
                    tag["href"] = href
            else:
                tag.attrs.clear()

        # HTMLã‚’æ–‡å­—åˆ—ã¨ã—ã¦å–å¾—ï¼ˆåœ§ç¸®å‰ã®çŠ¶æ…‹ï¼‰
        original_html_str = str(target_element)

        # åœ§ç¸®å‡¦ç†ï¼ˆHTMLæ§‹é€ ã¨æ”¹è¡Œã¯ç¶­æŒã€ä½™åˆ†ãªç©ºç™½ã®ã¿å‰Šé™¤ï¼‰
        compressed_content = original_html_str
        compressed_content = re.sub(
            r">\s+<", "><", compressed_content
        )  # ã‚¿ã‚°é–“ã®ç©ºç™½å‰Šé™¤

        # æœ€çµ‚çš„ãªåœ§ç¸®å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
        final_compressed_tokens = estimate_claude_tokens(compressed_content)

        # è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º
        print(f"å…ƒãƒšãƒ¼ã‚¸å…¨ä½“ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {original_full_tokens:,}")
        print(f"æŠ½å‡ºå¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {extracted_tokens:,}")
        print(f"æœ€çµ‚åœ§ç¸®å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {final_compressed_tokens:,}")
        print(f"æŠ½å‡ºã«ã‚ˆã‚‹å‰Šæ¸›: {original_full_tokens - extracted_tokens:,}")
        print(f"åœ§ç¸®ã«ã‚ˆã‚‹å‰Šæ¸›: {extracted_tokens - final_compressed_tokens:,}")
        print(f"ç·å‰Šæ¸›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {original_full_tokens - final_compressed_tokens:,}")

        # å…¨ä½“çš„ãªåœ§ç¸®ç‡ã®è¨ˆç®—ï¼ˆå…ƒãƒšãƒ¼ã‚¸å…¨ä½“ â†’ æœ€çµ‚åœ§ç¸®å¾Œï¼‰
        if original_full_tokens > 0:
            total_compression_ratio = (
                (original_full_tokens - final_compressed_tokens)
                / original_full_tokens
                * 100
            )
        else:
            total_compression_ratio = 0

        # æŠ½å‡ºã®ã¿ã®åŠ¹æœ
        if original_full_tokens > 0:
            extraction_ratio = (
                (original_full_tokens - extracted_tokens) / original_full_tokens * 100
            )
        else:
            extraction_ratio = 0

        # åœ§ç¸®ã®ã¿ã®åŠ¹æœ
        if extracted_tokens > 0:
            compression_only_ratio = (
                (extracted_tokens - final_compressed_tokens) / extracted_tokens * 100
            )
        else:
            compression_only_ratio = 0

        print(f"æŠ½å‡ºã«ã‚ˆã‚‹å‰Šæ¸›ç‡: {extraction_ratio:.2f}%")
        print(f"åœ§ç¸®ã«ã‚ˆã‚‹å‰Šæ¸›ç‡: {compression_only_ratio:.2f}%")
        print(f"ç·åˆåœ§ç¸®ç‡: {total_compression_ratio:.2f}%")

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰
        metadata_section = f"<h1>{title_text}</h1>\n"
        if source_url:
            metadata_section += f'<p><strong>URL:</strong> <a href="{source_url}">{source_url}</a></p>\n'
        if description_text:
            metadata_section += (
                f"<p><strong>ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³:</strong> {description_text}</p>\n"
            )
        metadata_section += "\n"

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨æœ¬æ–‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’çµåˆã—ã¦è¿”ç­”
        result = f"{metadata_section}{compressed_content}"

        return result

    except Exception as e:
        print(f"BeautifulSoupã§ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return ""


def main():
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    url = os.environ.get("URL")
    if not url:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆURLã‚’ä½¿ç”¨ï¼ˆé–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆç”¨ï¼‰
        url = "https://tech-lab.sios.jp/archives/48173"
        print(f"âš ï¸  URLæœªæŒ‡å®šã®ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆURLã‚’ä½¿ç”¨: {url}")

    # URLã®ç°¡å˜ãªæ¤œè¨¼
    if not url.startswith("https://tech-lab.sios.jp/archives"):
        raise ValueError(
            "URLã¯ 'https://tech-lab.sios.jp/archives' ã§å§‹ã¾ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
        )

    # blogãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
    cache_dir = "blog"
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)
        print(f"{cache_dir}ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸã€‚")

    # HTMLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ç”Ÿæˆï¼ˆURL -> ãƒ•ã‚¡ã‚¤ãƒ«åå¤‰æ›ï¼‰
    # ä¾‹: https://tech-lab.sios.jp/archives/48173 -> tech-lab-sios-jp-archives-48173.html
    domain_path = (
        url.replace("https://", "")
        .replace("http://", "")
        .replace("/", "-")
        .replace(".", "-")
    )
    html_file_path = f"{cache_dir}/{domain_path}.html"

    print(f"ğŸ“ HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å…ˆ: {html_file_path}")

    # htmlãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯å¾Œç¶šã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
    if os.path.exists(html_file_path):
        print(f"âœ… {html_file_path} ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ãŸã‚ã€å¾Œç¶šã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(html_file_path)} bytes")
        return

    # HTMLã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚¹
    print("ğŸ”„ HTMLå–å¾—ãƒ»ãƒ‘ãƒ¼ã‚¹é–‹å§‹...")
    soup = fetch_and_parse_html(url)

    if soup is None:
        raise RuntimeError("HTMLã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")

    # åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡ºã—ã¦è¡¨ç¤º
    title = soup.find("title")
    meta_description = soup.find("meta", attrs={"name": "description"})

    if title:
        print(f"ğŸ“„ ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {title.get_text().strip()}")
    if meta_description and meta_description.get("content"):
        desc_text = meta_description.get("content").strip()
        print(
            f"ğŸ“ ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³: {desc_text[:100]}{'...' if len(desc_text) > 100 else ''}"
        )

    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºãƒ»åœ§ç¸®
    print("ğŸ”§ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºãƒ»åœ§ç¸®é–‹å§‹...")
    result = extract_and_compress_content_bs4(soup, source_url=url)

    if not result:
        raise RuntimeError("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")

    # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    print(f"ğŸ’¾ HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­: {html_file_path}")
    try:
        with open(html_file_path, "w", encoding="utf-8") as f:
            f.write(result)

        # ä¿å­˜ç¢ºèª
        if os.path.exists(html_file_path):
            file_size = os.path.getsize(html_file_path)
            print(f"âœ… HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {html_file_path}")
            print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size} bytes")
        else:
            raise RuntimeError("HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")

    except Exception as e:
        print(f"âŒ HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        raise


if __name__ == "__main__":
    main()
